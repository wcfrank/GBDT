{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To explore the GBDT.compute_loss\n",
    "\n",
    "Binomial loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from math import exp, log\n",
    "\n",
    "from gbdt.data import DataSet\n",
    "from gbdt.model import GBDT\n",
    "from gbdt.model import BinomialDeviance, RegressionLossFunction\n",
    "from gbdt.tree import construct_decision_tree, Tree\n",
    "from gbdt.tree import MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DataSet('data/credit.data.csv')\n",
    "dataset.get_label_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The initial properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iter = 20\n",
    "sample_rate = 0.8\n",
    "learn_rate = 0.5\n",
    "max_depth = 7\n",
    "loss_type = 'binary-classification'\n",
    "split_points = 0\n",
    "trees = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function: Binary classification\n",
    "`GBDT.model.fit`: \n",
    "\n",
    "when `loss_type == 'binary-classification'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = BinomialDeviance(n_classes=dataset.get_label_size())\n",
    "loss.K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = dict()\n",
    "loss.initialize(f, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = dataset.get_instances_idset()\n",
    "# train_data is a set, which contains IDs from 1 to 653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define compute_loss                \n",
    "def compute_loss(dataset, subset, f):\n",
    "    total_loss = 0.0\n",
    "    if loss.K == 1:\n",
    "#         print(loss.K)\n",
    "        for id in dataset.get_instances_idset():\n",
    "            y_i = dataset.get_instance(id)['label']\n",
    "            f_value = f[id]\n",
    "            p_1 = 1/(1+exp(-2*f_value))\n",
    "            try:\n",
    "                total_loss -= ((1+y_i)*log(p_1)/2) + ((1-y_i)*log(1-p_1)/2) # Here we will explain deeper!\n",
    "            except ValueError as e:\n",
    "                print(y_i, p_1)\n",
    "    else:\n",
    "        for id in dataset.get_instances_idset():\n",
    "            instance = dataset.get_instance(id)\n",
    "            f_values = f[id]\n",
    "            exp_values = {}\n",
    "            for label in f_values:\n",
    "                print('label is:', label)\n",
    "                exp_values[label] = exp(f_values[label])\n",
    "            probs = {}\n",
    "            for label in f_values:\n",
    "                probs[label] = exp_values[label]/sum(exp_values.values())\n",
    "                # 预测的越准确则log(probs[instance[\"label\"]])越接近0 loss也就越小\n",
    "            total_loss -= log(probs[instance[\"label\"]])\n",
    "    return total_loss/dataset.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_instance_f_value(instance):\n",
    "    \"\"\"计算样本的f值\"\"\"\n",
    "    if loss.K == 1:\n",
    "        f_value = 0.0\n",
    "        for ite in trees:\n",
    "            f_value += learn_rate * trees[ite].get_predict_value(instance)\n",
    "    else:\n",
    "        f_value = dict()\n",
    "        for label in loss.labelset:\n",
    "            f_value[label] = 0.0\n",
    "        for ite in trees:\n",
    "            # 对于多分类问题，为每个类别构造一颗回归树\n",
    "            for label in loss.labelset:\n",
    "                tree = trees[ite][label]\n",
    "                f_value[label] += learn_rate*tree.get_predict_value(instance)\n",
    "    return f_value\n",
    "\n",
    "def predict(instance):\n",
    "    \"\"\"\n",
    "    对于回归和二元分类返回f值\n",
    "    对于多元分类返回每一类的f值\n",
    "    \"\"\"\n",
    "    return compute_instance_f_value(instance)\n",
    "\n",
    "def predict_prob(instance):\n",
    "    \"\"\"为了统一二元分类和多元分类，返回属于每个类别的概率\"\"\"\n",
    "    if isinstance(loss, RegressionLossFunction):\n",
    "        raise RuntimeError('regression problem can not predict prob ')\n",
    "    if loss.K == 1:\n",
    "        f_value = compute_instance_f_value(instance)\n",
    "        probs = dict()\n",
    "        probs['+1'] = 1/(1+exp(-2*f_value))\n",
    "        probs['-1'] = 1 - probs['+1']\n",
    "    else:\n",
    "        f_value = compute_instance_f_value(instance)\n",
    "        exp_values = dict()\n",
    "        for label in f_value:\n",
    "            exp_values[label] = exp(f_value[label])\n",
    "        exp_sum = sum(exp_values.values())\n",
    "        probs = dict()\n",
    "        # 归一化，并得到相应的概率值\n",
    "        for label in exp_values:\n",
    "            probs[label] = exp_values[label]/exp_sum\n",
    "    return probs\n",
    "\n",
    "def predict_label(instance):\n",
    "    \"\"\"预测标签\"\"\"\n",
    "    predict_label = None\n",
    "    if isinstance(loss, BinomialDeviance):\n",
    "        probs = predict_prob(instance)\n",
    "        predict_label = 1 if probs['+1'] >= probs['-1'] else -1\n",
    "    else:\n",
    "        probs = self.predict_prob(instance)\n",
    "        # 选出K分类中，概率值最大的label\n",
    "        for label in probs:\n",
    "            if not predict_label or probs[label] > probs[predict_label]:\n",
    "                predict_label = label\n",
    "    return predict_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GBDT.compute_loss`：\n",
    "\n",
    "如果`loss.K==1`,对应的是log loss，此时loss的计算公式为$loss = loss - likelihood$\n",
    "$$likelihood = \\bigg(\\frac{1+y_i}{2}\\log p + \\frac{1-y_i}{2}\\log(1-p)\\bigg)$$ \n",
    "其中，$f = f[id], p = \\frac{1}{1+\\exp(-2f)}$.\n",
    "\n",
    "回忆：在`GBDT.model.BinomialDeviance.compute_residual`中，定义loss为\n",
    "$$loss = \\frac{2y_i}{1+\\exp{(-2y_if)}}$$\n",
    "\n",
    "#### 问题：这两个loss等价嘛？\n",
    "我们从第一个表达式推到第二个：\n",
    "\n",
    "第一个loss是来自于logistic regression的log loss，label的取值是0,1；第二个loss是来自于Friedman的文章，label的取值是-1,1。所以需要将0,1取值变换为-1,1取值：\n",
    "\n",
    "$y = 2Y-1$，如果$Y\\in\\{0,1\\}$，则$y\\in\\{-1,1\\}$映射\n",
    "\n",
    "令$p = P(y=1|x)=\\frac{1}{1+\\exp(-2f)}$， \n",
    "$$\\begin{array}{rl}\n",
    "likelihood(Y,p) & = Y\\log p + (1-Y)\\log(1-p)\\\\\n",
    "     & = \\frac{1+y}{2}\\log p + \\frac{1-y}{2}\\log(1-p) \\\\\n",
    "     & = \\frac{1+y}{2}\\log\\frac{1}{1+\\exp(-2f)} + \\frac{1-y}{2}\\log \\frac{\\exp(-2f)}{1+ \\exp{(-2f)}}\\\\\n",
    "     & = \\frac{1+y}{2}\\log\\frac{1}{1+\\exp(-2f)} + \\frac{1-y}{2}\\log \\frac{1}{1+ \\exp{(2f)}}\\\\\n",
    "     & = -\\frac{1+y}{2}\\log\\big(1+e^{-2f}\\big) - \\frac{1-y}{2}\\log\\big({1+ e^{2f}}\\big)\\\\\n",
    "     & = -\\log(1-e^{-2yf}), ~~~(y=-1~or ~1)\n",
    "\\end{array}$$\n",
    "\n",
    "目标是极大化似然，等价于极小化负的似然，即极小化的损失函数：$\\min ~L(y, f(x)) = \\min ~\\log(1-e^{-2yf})$\n",
    "\n",
    "此时，$$f^*(x) = \\arg\\min\\limits_{f(x)} E_{y|x}\\bigg(\\log(1+e^{-2yf(x)})\\bigg) = \\frac{1}{2}\\log\\frac{P(y=1|x)}{P(y=-1|x)}$$\n",
    "为理论上$f(x)$的最优的映射。\n",
    "\n",
    "证明：因为$E(\\log(1+e^{-2yf(x)})) = P(y=1|x)\\log(1+e^{-2f}) + P(y=-1|x)\\log(1+e^{2f})$\n",
    "\n",
    "所以\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\frac{\\partial E(\\log(1+e^{-2yf}))}{\\partial f} & = P(y=1|x)\\frac{-2e^{-2f}}{1+e^{-2f}} + P(y=-1|x)\\frac{2e^{2f}}{1+e^{2f}} \\\\\n",
    "                                                & \\propto P(y=1|x)\\frac{-1}{1+e^{2f}} + P(y=-1|x)\\frac{e^{2f}}{1+e^{2f}} \\\\\n",
    "                                                & \\propto -P(y=1|x) + e^{2f}P(y=-1|x) \\\\\n",
    "                                                & = 0\n",
    "\\end{array}\n",
    "$$\n",
    "所以$f^* = \\frac{1}{2}\\log\\frac{P(y=1|x)}{P(y=-1|x)}$\n",
    "\n",
    "参考资料：\n",
    "- http://statweb.stanford.edu/~tibs/book/chap14.pdf Page10\n",
    "- http://docs.salford-systems.com/GreedyFuncApproxSS.pdf Page8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.3850905347256806\n",
      "2 0.2479308474226447\n",
      "3 0.1688799662267075\n",
      "4 0.11971562470711063\n",
      "5 0.09054012247466625\n",
      "6 0.06904249771756119\n",
      "7 0.05364686501770368\n",
      "8 0.043670593958652235\n",
      "9 0.034551810011379226\n",
      "10 0.0268865680632009\n",
      "11 0.023609457248059188\n",
      "12 0.019432939290180737\n",
      "13 0.017353362121343054\n",
      "14 0.014058836244390267\n",
      "15 0.012388793007838022\n",
      "16 0.010032983389178995\n",
      "17 0.008900641635052671\n",
      "18 0.007912802950569075\n",
      "19 0.006688882609641617\n",
      "20 0.0055106738485271875\n"
     ]
    }
   ],
   "source": [
    "for iter_m in range(1, max_iter+1): # Chao: 用决策树拟合的步数 m\n",
    "    subset = train_data\n",
    "    if 0 < sample_rate < 1:\n",
    "        # Chao： 这里只选择80%的Id来构造决策树。\n",
    "        # Chao：未来，剩下20%的Id只是通过这个构造好的决策树的leafnodes得到取值\n",
    "        subset = sample(subset, int(len(subset)*sample_rate))\n",
    "    # 用损失函数的负梯度作为回归问题提升树的残差近似值\n",
    "    residual = loss.compute_residual(dataset, subset, f)\n",
    "    leaf_nodes = []\n",
    "    targets = residual\n",
    "    tree = construct_decision_tree(dataset, subset, targets, 0, leaf_nodes, max_depth, loss, split_points)\n",
    "    trees[iter_m] = tree\n",
    "    loss.update_f_value(f, tree, leaf_nodes, subset, dataset, learn_rate) # Chao：更新每个Id（样本点）的值\n",
    "    if isinstance(loss, RegressionLossFunction):\n",
    "        # todo 判断回归的效果\n",
    "        pass\n",
    "    else:\n",
    "        train_loss = compute_loss(dataset, train_data, f)\n",
    "#         print(\"iter_m%d : train loss=%f\" % (iter_m,train_loss))\n",
    "        print(iter_m, train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.325045467075996"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.579335791032858"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.0914734636045216"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[1]\n",
    "f[2]\n",
    "f[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ins = dataset.get_instance(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A1': 'b',\n",
       " 'A10': 't',\n",
       " 'A11': 1.0,\n",
       " 'A12': 'f',\n",
       " 'A13': 'g',\n",
       " 'A14': 202.0,\n",
       " 'A15': 0.0,\n",
       " 'A2': 30.83,\n",
       " 'A3': 0.0,\n",
       " 'A4': 'u',\n",
       " 'A5': 'g',\n",
       " 'A6': 'w',\n",
       " 'A7': 'v',\n",
       " 'A8': 1.25,\n",
       " 'A9': 't',\n",
       " 'label': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.325045467075996"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_instance_f_value(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.325045467075996"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+1': 0.9905298094644651, '-1': 0.009470190535534884}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_prob(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label(ins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter1 : train loss=0.397886\n",
      "iter2 : train loss=0.254912\n",
      "iter3 : train loss=0.177351\n",
      "iter4 : train loss=0.133279\n",
      "iter5 : train loss=0.100853\n",
      "iter6 : train loss=0.072687\n",
      "iter7 : train loss=0.057461\n",
      "iter8 : train loss=0.045802\n",
      "iter9 : train loss=0.037130\n",
      "iter10 : train loss=0.030485\n",
      "iter11 : train loss=0.025559\n",
      "iter12 : train loss=0.019611\n",
      "iter13 : train loss=0.015378\n",
      "iter14 : train loss=0.014205\n",
      "iter15 : train loss=0.012190\n",
      "iter16 : train loss=0.009855\n",
      "iter17 : train loss=0.008412\n",
      "iter18 : train loss=0.007138\n",
      "iter19 : train loss=0.005737\n",
      "iter20 : train loss=0.004679\n"
     ]
    }
   ],
   "source": [
    "# data_file = './data/credit.data.csv'\n",
    "# dateset = DataSet(data_file)\n",
    "gbdt = GBDT(max_iter=20, sample_rate=0.8, learn_rate=0.5, max_depth=7, loss_type='binary-classification')\n",
    "gbdt.fit(dataset, dataset.get_instances_idset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4720614878340497"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins = dataset.get_instance(1)\n",
    "gbdt.compute_instance_f_value(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+1': 0.9929252478828322, '-1': 0.007074752117167793}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbdt.predict_prob(ins)\n",
    "gbdt.predict_label(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
