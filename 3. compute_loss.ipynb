{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To explore the GBDT.compute_loss\n",
    "\n",
    "Binomial loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from math import exp, log\n",
    "\n",
    "from gbdt.data import DataSet\n",
    "from gbdt.model import GBDT\n",
    "from gbdt.model import BinomialDeviance, RegressionLossFunction\n",
    "from gbdt.tree import construct_decision_tree, Tree\n",
    "from gbdt.tree import MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DataSet('data/credit.data.csv')\n",
    "dataset.get_label_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The initial properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iter = 20\n",
    "sample_rate = 0.8\n",
    "learn_rate = 0.5\n",
    "max_depth = 7\n",
    "loss_type = 'binary-classification'\n",
    "split_points = 0\n",
    "trees = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function: Binary classification\n",
    "`GBDT.model.fit`: \n",
    "\n",
    "when `loss_type == 'binary-classification'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = BinomialDeviance(n_classes=dataset.get_label_size())\n",
    "loss.K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = dict()\n",
    "loss.initialize(f, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = dataset.get_instances_idset()\n",
    "# train_data is a set, which contains IDs from 1 to 653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define compute_loss                \n",
    "def compute_loss(dataset, subset, f):\n",
    "    total_loss = 0.0\n",
    "    if loss.K == 1:\n",
    "        print(loss.K)\n",
    "        for id in dataset.get_instances_idset():\n",
    "            y_i = dataset.get_instance(id)['label']\n",
    "            f_value = f[id]\n",
    "            p_1 = 1/(1+exp(-2*f_value))\n",
    "            try:\n",
    "                total_loss -= ((1+y_i)*log(p_1)/2) + ((1-y_i)*log(1-p_1)/2) # Here we will explain deeper!\n",
    "            except ValueError as e:\n",
    "                print(y_i, p_1)\n",
    "    else:\n",
    "        for id in dataset.get_instances_idset():\n",
    "            instance = dataset.get_instance(id)\n",
    "            f_values = f[id]\n",
    "            exp_values = {}\n",
    "            for label in f_values:\n",
    "                print('label is:', label)\n",
    "                exp_values[label] = exp(f_values[label])\n",
    "            probs = {}\n",
    "            for label in f_values:\n",
    "                probs[label] = exp_values[label]/sum(exp_values.values())\n",
    "                # 预测的越准确则log(probs[instance[\"label\"]])越接近0 loss也就越小\n",
    "            total_loss -= log(probs[instance[\"label\"]])\n",
    "    return total_loss/dataset.size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GBDT.compute_loss`：\n",
    "\n",
    "如果`loss.K==1`,对应的是log loss，此时loss的计算公式为$loss = loss - likelihood$\n",
    "$$likelihood = \\bigg(\\frac{1+y_i}{2}\\log p + \\frac{1-y_i}{2}\\log(1-p)\\bigg)$$ \n",
    "其中，$f = f[id], p = \\frac{1}{1+\\exp(-2f)}$.\n",
    "\n",
    "回忆：在`GBDT.model.BinomialDeviance.compute_residual`中，定义loss为\n",
    "$$loss = \\frac{2y_i}{1+\\exp{(-2y_if)}}$$\n",
    "\n",
    "#### 问题：这两个loss等价嘛？\n",
    "我们从第一个表达式推到第二个：\n",
    "\n",
    "第一个loss是来自于logistic regression的log loss，label的取值是0,1；第二个loss是来自于Friedman的文章，label的取值是-1,1。所以需要将0,1取值变换为-1,1取值：\n",
    "\n",
    "$y = 2Y-1$，如果$Y\\in\\{0,1\\}$，则$y\\in\\{-1,1\\}$映射\n",
    "\n",
    "令$p = P(y=1|x)=\\frac{1}{1+\\exp(-2f)}$， \n",
    "$$\\begin{array}{rl}\n",
    "likelihood(Y,p) & = Y\\log p + (1-Y)\\log(1-p)\\\\\n",
    "     & = \\frac{1+y}{2}\\log p + \\frac{1-y}{2}\\log(1-p) \\\\\n",
    "     & = \\frac{1+y}{2}\\log\\frac{1}{1+\\exp(-2f)} + \\frac{1-y}{2}\\log \\frac{\\exp(-2f)}{1+ \\exp{(-2f)}}\\\\\n",
    "     & = \\frac{1+y}{2}\\log\\frac{1}{1+\\exp(-2f)} + \\frac{1-y}{2}\\log \\frac{1}{1+ \\exp{(2f)}}\\\\\n",
    "     & = -\\frac{1+y}{2}\\log\\big(1+e^{-2f}\\big) - \\frac{1-y}{2}\\log\\big({1+ e^{2f}}\\big)\\\\\n",
    "     & = -\\log(1-e^{-2yf}), ~~~(y=-1~or ~1)\n",
    "\\end{array}$$\n",
    "\n",
    "目标是极大化似然，等价于极小化负的似然，即极小化的损失函数：$\\min ~L(y, f(x)) = \\min ~\\log(1-e^{-2yf})$\n",
    "\n",
    "此时，$$f^*(x) = \\arg\\min\\limits_{f(x)} E_{y|x}\\bigg(\\log(1+e^{-2yf(x)})\\bigg) = \\frac{1}{2}\\log\\frac{P(y=1|x)}{P(y=-1|x)}$$\n",
    "为理论上$f(x)$的最优的映射。\n",
    "\n",
    "证明：因为$E(\\log(1+e^{-2yf(x)})) = P(y=1|x)\\log(1+e^{-2f}) + P(y=-1|x)\\log(1+e^{2f})$\n",
    "\n",
    "所以\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\frac{\\partial E(\\log(1+e^{-2yf}))}{\\partial f} & = P(y=1|x)\\frac{-2e^{-2f}}{1+e^{-2f}} + P(y=-1|x)\\frac{2e^{2f}}{1+e^{2f}} \\\\\n",
    "                                                & \\propto P(y=1|x)\\frac{-1}{1+e^{2f}} + P(y=-1|x)\\frac{e^{2f}}{1+e^{2f}} \\\\\n",
    "                                                & \\propto -P(y=1|x) + e^{2f}P(y=-1|x) \\\\\n",
    "                                                & = 0\n",
    "\\end{array}\n",
    "$$\n",
    "所以$f^* = \\frac{1}{2}\\log\\frac{P(y=1|x)}{P(y=-1|x)}$\n",
    "\n",
    "参考资料：\n",
    "- http://statweb.stanford.edu/~tibs/book/chap14.pdf Page10\n",
    "- http://docs.salford-systems.com/GreedyFuncApproxSS.pdf Page8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1 0.3897065340426447\n",
      "1\n",
      "2 0.25475566741135086\n",
      "1\n",
      "3 0.16685470123077706\n",
      "1\n",
      "4 0.12415213494532737\n",
      "1\n",
      "5 0.09177197718441253\n",
      "1\n",
      "6 0.06470819435044439\n",
      "1\n",
      "7 0.055316774649429835\n",
      "1\n",
      "8 0.04360475947332747\n",
      "1\n",
      "9 0.03692395239087183\n",
      "1\n",
      "10 0.029970000056618883\n",
      "1\n",
      "11 0.026079538760605498\n",
      "1\n",
      "12 0.020691391394846177\n",
      "1\n",
      "13 0.016758166111156315\n",
      "1\n",
      "14 0.014348610847863588\n",
      "1\n",
      "15 0.011982645998606168\n",
      "1\n",
      "16 0.009691216866636843\n",
      "1\n",
      "17 0.008757701950884659\n",
      "1\n",
      "18 0.007690325471064932\n",
      "1\n",
      "19 0.006536597394716198\n",
      "1\n",
      "20 0.00548404731063743\n"
     ]
    }
   ],
   "source": [
    "for iter_m in range(1, max_iter+1): # Chao: 用决策树拟合的步数 m\n",
    "    subset = train_data\n",
    "    if 0 < sample_rate < 1:\n",
    "        # Chao： 这里只选择80%的Id来构造决策树。\n",
    "        # Chao：未来，剩下20%的Id只是通过这个构造好的决策树的leafnodes得到取值\n",
    "        subset = sample(subset, int(len(subset)*sample_rate))\n",
    "    # 用损失函数的负梯度作为回归问题提升树的残差近似值\n",
    "    residual = loss.compute_residual(dataset, subset, f)\n",
    "    leaf_nodes = []\n",
    "    targets = residual\n",
    "    tree = construct_decision_tree(dataset, subset, targets, 0, leaf_nodes, max_depth, loss, split_points)\n",
    "    trees[iter_m] = tree\n",
    "    loss.update_f_value(f, tree, leaf_nodes, subset, dataset, learn_rate) # Chao：更新每个Id（样本点）的值\n",
    "    if isinstance(loss, RegressionLossFunction):\n",
    "        # todo 判断回归的效果\n",
    "        pass\n",
    "    else:\n",
    "        train_loss = compute_loss(dataset, train_data, f)\n",
    "#         print(\"iter_m%d : train loss=%f\" % (iter_m,train_loss))\n",
    "        print(iter_m, train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iter_m = 1\n",
    "\n",
    "# subset = train_data\n",
    "# if 0 < sample_rate < 1:\n",
    "#     subset = sample(subset, int(len(subset)*sample_rate))\n",
    "# residual = loss.compute_residual(dataset, subset, f)\n",
    "# leaf_nodes = []\n",
    "# targets = residual\n",
    "# tree = construct_decision_tree(dataset, subset, targets, 0, leaf_nodes, max_depth, loss, split_points)\n",
    "# trees[iter_m] = tree\n",
    "# loss.update_f_value(f, tree, leaf_nodes, subset, dataset, learn_rate) # Chao：更新每个Id（样本点）的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # the content of compute_loss\n",
    "# for id in dataset.get_instances_idset():\n",
    "#     instance = dataset.get_instance(id)\n",
    "#     f_values = f[id]\n",
    "#     print(id, f_values)\n",
    "#     exp_values = {}\n",
    "#     for label in f_values:\n",
    "#         exp_values[label] = exp(f_values[label])\n",
    "#         print(exp_values[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     for label in f_values:\n",
    "#         exp_values[label] = exp(f_values[label])\n",
    "#     probs = {}\n",
    "#     for label in f_values:\n",
    "#         probs[label] = exp_values[label]/sum(exp_values.values())\n",
    "#     total_loss -= log(probs[instance[\"label\"]])\n",
    "# # return total_loss/dataset.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
